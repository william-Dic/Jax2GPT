# JAX2GPT: From-Scratch GPT Implementation Using JAX

This repository is a from-scratch implementation of GPT models using **JAX** and **Flax**. The project starts with **GPT (bigram model), GPT-2, and GPT-3** (if compute resources allow) and will eventually include a full **reimplementation of Deepseek-R1-distilled-7B** from scratch using JAX and Flax.  

## **Repository Roadmap**
1. **GPT (Bigram Model)** → A minimal character-level Transformer.
2. **GPT-2** → Implements a full GPT-2-like Transformer with multi-head attention.
3. **GPT-3** → Scaling up with larger context windows and parameter tuning.
4. **Deepseek-R1-distilled-7B** → From scratch JAX-based reimplementation.

---

## **Installation**
Clone the repository and install dependencies:
```bash
git clone https://github.com/william-Dic/Jax2GPT.git
```

## **License**
Apache 2.0
