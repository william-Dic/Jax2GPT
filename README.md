# JAX2GPT: Building GPT from Scratch with JAX/Flax

Welcome to **JAX2GPT**, a project dedicated to implementing GPT models entirely from scratch using [JAX](https://github.com/google/jax) and [Flax](https://github.com/google/flax). This repository serves as a playground for developing various GPT architectures—from a simple bigram model to more advanced models—and ultimately, a full reimplementation of Deepseek-R1-distilled-7B.

## Repository Roadmap

1. **GPT (Bigram Model)**  
   A minimal character-level Transformer serving as the foundational building block.

2. **GPT-2**  
   A full GPT-2-like Transformer model featuring multi-head attention mechanisms.

3. **GPT-3**  
   Scaling up the model with larger context windows and advanced parameter tuning (compute permitting).

4. **Deepseek-R1-distilled-7B**  
   A comprehensive from-scratch reimplementation of Deepseek-R1-distilled-7B using JAX/Flax.

---

## Installation

Clone the repository and install the required dependencies:

```bash
git clone https://github.com/william-Dic/Jax2GPT.git
cd Jax2GPT
# Follow the instructions in the repository for installing dependencies
```

## License

This project is licensed under the Apache 2.0 License.
