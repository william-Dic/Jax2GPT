{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade jax[cuda] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install --upgrade tensorflow tensorflow-probability\n",
    "!pip install datasets\n",
    "\n",
    "!pip install --upgrade \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "!pip install --upgrade tensorflow tensorflow-probability\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Hyperparameters and setup\n",
    "# -----------------------------------------------------------------------------\n",
    "batch_size = 64          # number of independent sequences processed in parallel\n",
    "block_size = 256         # maximum context length for predictions\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "learning_rate = 3e-4\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout_rate = 0.2\n",
    "seed = 1337\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# -----------------------------------------------------------------------------\n",
    "# Read the text (e.g. tiny Shakespeare)\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Create vocabulary and encoding/decoding functions\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Encode the full text as a numpy array of integers\n",
    "data = np.array(encode(text), dtype=np.int32)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of data as JAX arrays of inputs (x) and targets (y).\"\"\"\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = np.random.randint(0, len(data_split) - block_size, size=(batch_size,))\n",
    "    x = np.stack([data_split[i : i + block_size] for i in ix])\n",
    "    y = np.stack([data_split[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return jnp.array(x), jnp.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Model Definition using Flax\n",
    "# -----------------------------------------------------------------------------\n",
    "class Head(nn.Module):\n",
    "    head_size: int\n",
    "    dropout_rate: float = dropout_rate\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, deterministic: bool = False):\n",
    "        # x: (B, T, C)\n",
    "        B, T, _ = x.shape\n",
    "        # Compute key, query, and value projections\n",
    "        k = nn.Dense(self.head_size, use_bias=False)(x)\n",
    "        q = nn.Dense(self.head_size, use_bias=False)(x)\n",
    "        v = nn.Dense(self.head_size, use_bias=False)(x)\n",
    "        scale = 1.0 / jnp.sqrt(self.head_size)\n",
    "        # Compute attention scores\n",
    "        wei = jnp.matmul(q, jnp.swapaxes(k, -2, -1)) * scale  # (B, T, T)\n",
    "        # Create a lower-triangular mask\n",
    "        mask = jnp.tril(jnp.ones((T, T), dtype=bool))\n",
    "        wei = jnp.where(mask, wei, -1e10)\n",
    "        wei = nn.softmax(wei, axis=-1)\n",
    "        wei = nn.Dropout(rate=self.dropout_rate)(wei, deterministic=deterministic)\n",
    "        # Weighted aggregation of the values\n",
    "        out = jnp.matmul(wei, v)  # (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    head_size: int\n",
    "    emb_dim: int\n",
    "    dropout_rate: float = dropout_rate\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, deterministic: bool = False):\n",
    "        heads = [\n",
    "            Head(self.head_size, dropout_rate=self.dropout_rate)(x, deterministic=deterministic)\n",
    "            for _ in range(self.num_heads)\n",
    "        ]\n",
    "        # Concatenate along the feature dimension\n",
    "        out = jnp.concatenate(heads, axis=-1)\n",
    "        out = nn.Dense(self.emb_dim)(out)\n",
    "        out = nn.Dropout(rate=self.dropout_rate)(out, deterministic=deterministic)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    emb_dim: int\n",
    "    dropout_rate: float = dropout_rate\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, deterministic: bool = False):\n",
    "        x = nn.Dense(4 * self.emb_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.emb_dim)(x)\n",
    "        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    emb_dim: int\n",
    "    num_heads: int\n",
    "    dropout_rate: float = dropout_rate\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, deterministic: bool = False):\n",
    "        # Self-attention with residual connection\n",
    "        x = x + MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            head_size=self.emb_dim // self.num_heads,\n",
    "            emb_dim=self.emb_dim,\n",
    "            dropout_rate=self.dropout_rate\n",
    "        )(nn.LayerNorm()(x), deterministic=deterministic)\n",
    "        # Feed-forward network with residual connection\n",
    "        x = x + FeedForward(emb_dim=self.emb_dim, dropout_rate=self.dropout_rate)(\n",
    "            nn.LayerNorm()(x), deterministic=deterministic\n",
    "        )\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    emb_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    block_size: int\n",
    "    dropout_rate: float = dropout_rate\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, idx, targets=None, deterministic: bool = False):\n",
    "        # idx: (B, T) of token indices\n",
    "        B, T = idx.shape\n",
    "        # Token and positional embeddings\n",
    "        tok_emb = nn.Embed(num_embeddings=self.vocab_size, features=self.emb_dim)(idx)\n",
    "        pos_emb = nn.Embed(num_embeddings=self.block_size, features=self.emb_dim)(\n",
    "            jnp.arange(T)\n",
    "        )\n",
    "        x = tok_emb + pos_emb[None, :, :]\n",
    "        # Transformer blocks\n",
    "        for _ in range(self.num_layers):\n",
    "            x = Block(emb_dim=self.emb_dim, num_heads=self.num_heads, dropout_rate=self.dropout_rate)(\n",
    "                x, deterministic=deterministic\n",
    "            )\n",
    "        x = nn.LayerNorm()(x)\n",
    "        logits = nn.Dense(self.vocab_size)(x)  # (B, T, vocab_size)\n",
    "        if targets is not None:\n",
    "            # Flatten the logits and targets for computing the loss\n",
    "            logits = logits.reshape(-1, self.vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, params, idx, max_new_tokens, rng):\n",
    "        \"\"\"Generate new text tokens given a starting context.\"\"\"\n",
    "        # idx: (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size :]\n",
    "            logits, _ = self.apply({'params': params}, idx_cond, deterministic=True)\n",
    "            logits = logits[:, -1, :]  # (B, vocab_size) for the last time step\n",
    "            # Sample from the distribution\n",
    "            next_token = jax.random.categorical(rng, logits)[:, None]\n",
    "            idx = jnp.concatenate([idx, next_token], axis=1)\n",
    "            rng, _ = jax.random.split(rng)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training State and Step Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, x, y, dropout_rng):\n",
    "    def loss_fn(params):\n",
    "        _, loss = model.apply({'params': params}, x, targets=y, deterministic=False, rngs={'dropout': dropout_rng})\n",
    "        return loss\n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, x, y):\n",
    "    _, loss = model.apply({'params': params}, x, targets=y, deterministic=True)\n",
    "    return loss\n",
    "\n",
    "def estimate_loss(state):\n",
    "    losses = {'train': [], 'val': []}\n",
    "    for split in ['train', 'val']:\n",
    "        for _ in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            loss = eval_step(state.params, x, y)\n",
    "            losses[split].append(loss)\n",
    "        losses[split] = np.mean([l.item() for l in losses[split]])\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Main Training Loop\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Initialize the model and training state\n",
    "    model = GPTLanguageModel(\n",
    "        vocab_size=vocab_size,\n",
    "        emb_dim=n_embd,\n",
    "        num_layers=n_layer,\n",
    "        num_heads=n_head,\n",
    "        block_size=block_size,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    dummy_input = jnp.ones((batch_size, block_size), dtype=jnp.int32)\n",
    "    initial_variables = model.init(key, dummy_input, targets=dummy_input, deterministic=False)\n",
    "    params = initial_variables['params']\n",
    "    tx = optax.adamw(learning_rate)\n",
    "    state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "    # Training loop\n",
    "    for iter in range(max_iters):\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(state)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        x, y = get_batch('train')\n",
    "        key, dropout_rng = jax.random.split(key)\n",
    "        state = train_step(state, x, y, dropout_rng)\n",
    "\n",
    "    # Generate text from the trained model\n",
    "    context = jnp.zeros((1, 1), dtype=jnp.int32)  # starting with the token index 0\n",
    "    key, gen_rng = jax.random.split(key)\n",
    "    generated = model.generate(state.params, context, max_new_tokens=500, rng=gen_rng)\n",
    "    print(decode(np.array(generated[0])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
