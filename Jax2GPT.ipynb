{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n",
        "Install JAX with GPU CUDA support (in this case, CUDA 11 and cuDNN 8.6)."
      ],
      "metadata": {
        "id": "YXYjTSIVcOSP"
      },
      "id": "YXYjTSIVcOSP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell1",
        "outputId": "c9d7e5a0-e27c-4806-9759-9178b60416a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda] in /usr/local/lib/python3.11/dist-packages (0.4.33)\n",
            "Collecting jax[cuda]\n",
            "  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.2,>=0.5.1 (from jax[cuda])\n",
            "  Downloading jaxlib-0.5.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax[cuda]) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax[cuda]) (1.26.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax[cuda]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax[cuda]) (1.13.1)\n",
            "Collecting jax-cuda12-plugin<=0.5.2,>=0.5.1 (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda])\n",
            "  Downloading jax_cuda12_plugin-0.5.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax-cuda12-pjrt==0.5.1 (from jax-cuda12-plugin<=0.5.2,>=0.5.1->jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda])\n",
            "  Downloading jax_cuda12_pjrt-0.5.1-py3-none-manylinux2014_x86_64.whl.metadata (348 bytes)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda]) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda]) (12.5.82)\n",
            "Collecting nvidia-cuda-nvcc-cu12>=12.6.85 (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda])\n",
            "  Downloading nvidia_cuda_nvcc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda]) (9.3.0.75)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda]) (11.2.3.61)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda]) (11.6.3.83)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda]) (12.5.1.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.2,>=0.5.1; extra == \"cuda\"->jax[cuda]) (12.5.82)\n",
            "Downloading jaxlib-0.5.1-cp311-cp311-manylinux2014_x86_64.whl (105.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.5.1-cp311-cp311-manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.5.1-py3-none-manylinux2014_x86_64.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.5.2-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (40.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jax-cuda12-pjrt, nvidia-cuda-nvcc-cu12, jax-cuda12-plugin, jaxlib, jax\n",
            "  Attempting uninstall: jax-cuda12-pjrt\n",
            "    Found existing installation: jax-cuda12-pjrt 0.4.33\n",
            "    Uninstalling jax-cuda12-pjrt-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-pjrt-0.4.33\n",
            "  Attempting uninstall: nvidia-cuda-nvcc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvcc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvcc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvcc-cu12-12.5.82\n",
            "  Attempting uninstall: jax-cuda12-plugin\n",
            "    Found existing installation: jax-cuda12-plugin 0.4.33\n",
            "    Uninstalling jax-cuda12-plugin-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-plugin-0.4.33\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "Successfully installed jax-0.5.2 jax-cuda12-pjrt-0.5.1 jax-cuda12-plugin-0.5.1 jaxlib-0.5.1 nvidia-cuda-nvcc-cu12-12.8.93\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (3.1.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (0.1.9)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability) (25.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade jax[cuda] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install --upgrade tensorflow tensorflow-probability\n",
        "!pip install datasets\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TPU-equivalent installation code"
      ],
      "metadata": {
        "id": "EYsCvI4IcQsx"
      },
      "id": "EYsCvI4IcQsx"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install --upgrade tensorflow tensorflow-probability\n",
        "!pip install datasets\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "8byqcgdNcRM5"
      },
      "id": "8byqcgdNcRM5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters and setup"
      ],
      "metadata": {
        "id": "BDPBQ9nZcZG5"
      },
      "id": "BDPBQ9nZcZG5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell2",
      "metadata": {
        "id": "cell2"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "\n",
        "batch_size = 64          # number of independent sequences processed in parallel\n",
        "block_size = 256         # maximum context length for predictions\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "learning_rate = 3e-4\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout_rate = 0.2\n",
        "seed = 1337\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(seed)\n",
        "key = jax.random.PRNGKey(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "z-DQn3f-cfD_"
      },
      "id": "z-DQn3f-cfD_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell3",
      "metadata": {
        "id": "cell3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Read the text (e.g. tiny Shakespeare)\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create vocabulary and encoding/decoding functions\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Encode the full text as a numpy array of integers\n",
        "data = np.array(encode(text), dtype=np.int32)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    \"\"\"Generate a small batch of data as JAX arrays of inputs (x) and targets (y).\"\"\"\n",
        "    data_split = train_data if split == 'train' else val_data\n",
        "    ix = np.random.randint(0, len(data_split) - block_size, size=(batch_size,))\n",
        "    x = np.stack([data_split[i : i + block_size] for i in ix])\n",
        "    y = np.stack([data_split[i + 1 : i + block_size + 1] for i in ix])\n",
        "    return jnp.array(x), jnp.array(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition using Flax"
      ],
      "metadata": {
        "id": "Q3rhnKZ4cilm"
      },
      "id": "Q3rhnKZ4cilm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell4",
      "metadata": {
        "id": "cell4"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Head(nn.Module):\n",
        "    head_size: int\n",
        "    dropout_rate: float = dropout_rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic: bool = False):\n",
        "        # x: (B, T, C)\n",
        "        B, T, _ = x.shape\n",
        "        # Compute key, query, and value projections\n",
        "        k = nn.Dense(self.head_size, use_bias=False)(x)\n",
        "        q = nn.Dense(self.head_size, use_bias=False)(x)\n",
        "        v = nn.Dense(self.head_size, use_bias=False)(x)\n",
        "        scale = 1.0 / jnp.sqrt(self.head_size)\n",
        "        # Compute attention scores\n",
        "        wei = jnp.matmul(q, jnp.swapaxes(k, -2, -1)) * scale  # (B, T, T)\n",
        "        # Create a lower-triangular mask\n",
        "        mask = jnp.tril(jnp.ones((T, T), dtype=bool))\n",
        "        wei = jnp.where(mask, wei, -1e10)\n",
        "        wei = nn.softmax(wei, axis=-1)\n",
        "        wei = nn.Dropout(rate=self.dropout_rate)(wei, deterministic=deterministic)\n",
        "        # Weighted aggregation of the values\n",
        "        out = jnp.matmul(wei, v)  # (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    num_heads: int\n",
        "    head_size: int\n",
        "    emb_dim: int\n",
        "    dropout_rate: float = dropout_rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic: bool = False):\n",
        "        heads = [\n",
        "            Head(self.head_size, dropout_rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "            for _ in range(self.num_heads)\n",
        "        ]\n",
        "        # Concatenate along the feature dimension\n",
        "        out = jnp.concatenate(heads, axis=-1)\n",
        "        out = nn.Dense(self.emb_dim)(out)\n",
        "        out = nn.Dropout(rate=self.dropout_rate)(out, deterministic=deterministic)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    emb_dim: int\n",
        "    dropout_rate: float = dropout_rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic: bool = False):\n",
        "        x = nn.Dense(4 * self.emb_dim)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(self.emb_dim)(x)\n",
        "        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    emb_dim: int\n",
        "    num_heads: int\n",
        "    dropout_rate: float = dropout_rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic: bool = False):\n",
        "        # Self-attention with residual connection\n",
        "        x = x + MultiHeadAttention(\n",
        "            num_heads=self.num_heads,\n",
        "            head_size=self.emb_dim // self.num_heads,\n",
        "            emb_dim=self.emb_dim,\n",
        "            dropout_rate=self.dropout_rate\n",
        "        )(nn.LayerNorm()(x), deterministic=deterministic)\n",
        "        # Feed-forward network with residual connection\n",
        "        x = x + FeedForward(emb_dim=self.emb_dim, dropout_rate=self.dropout_rate)(\n",
        "            nn.LayerNorm()(x), deterministic=deterministic\n",
        "        )\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    vocab_size: int\n",
        "    emb_dim: int\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    block_size: int\n",
        "    dropout_rate: float = dropout_rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, idx, targets=None, deterministic: bool = False):\n",
        "        # idx: (B, T) of token indices\n",
        "        B, T = idx.shape\n",
        "        # Token and positional embeddings\n",
        "        tok_emb = nn.Embed(num_embeddings=self.vocab_size, features=self.emb_dim)(idx)\n",
        "        pos_emb = nn.Embed(num_embeddings=self.block_size, features=self.emb_dim)(\n",
        "            jnp.arange(T)\n",
        "        )\n",
        "        x = tok_emb + pos_emb[None, :, :]\n",
        "        # Transformer blocks\n",
        "        for _ in range(self.num_layers):\n",
        "            x = Block(emb_dim=self.emb_dim, num_heads=self.num_heads, dropout_rate=self.dropout_rate)(\n",
        "                x, deterministic=deterministic\n",
        "            )\n",
        "        x = nn.LayerNorm()(x)\n",
        "        logits = nn.Dense(self.vocab_size)(x)  # (B, T, vocab_size)\n",
        "        if targets is not None:\n",
        "            # Flatten the logits and targets for computing the loss\n",
        "            logits = logits.reshape(-1, self.vocab_size)\n",
        "            targets = targets.reshape(-1)\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
        "        else:\n",
        "            loss = None\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, params, idx, max_new_tokens, rng):\n",
        "        \"\"\"Generate new text tokens given a starting context.\"\"\"\n",
        "        # idx: (B, T)\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop to the last block_size tokens\n",
        "            idx_cond = idx[:, -self.block_size :]\n",
        "            logits, _ = self.apply({'params': params}, idx_cond, deterministic=True)\n",
        "            logits = logits[:, -1, :]  # (B, vocab_size) for the last time step\n",
        "            # Sample from the distribution\n",
        "            next_token = jax.random.categorical(rng, logits)[:, None]\n",
        "            idx = jnp.concatenate([idx, next_token], axis=1)\n",
        "            rng, _ = jax.random.split(rng)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training State and Step Functions"
      ],
      "metadata": {
        "id": "wyjCdDrrclXE"
      },
      "id": "wyjCdDrrclXE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell5",
      "metadata": {
        "id": "cell5"
      },
      "outputs": [],
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "    pass\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, x, y, dropout_rng):\n",
        "    def loss_fn(params):\n",
        "        _, loss = model.apply({'params': params}, x, targets=y, deterministic=False, rngs={'dropout': dropout_rng})\n",
        "        return loss\n",
        "    grads = jax.grad(loss_fn)(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(params, x, y):\n",
        "    _, loss = model.apply({'params': params}, x, targets=y, deterministic=True)\n",
        "    return loss\n",
        "\n",
        "def estimate_loss(state):\n",
        "    losses = {'train': [], 'val': []}\n",
        "    for split in ['train', 'val']:\n",
        "        for _ in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            loss = eval_step(state.params, x, y)\n",
        "            losses[split].append(loss)\n",
        "        losses[split] = np.mean([l.item() for l in losses[split]])\n",
        "    return losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Training Loop"
      ],
      "metadata": {
        "id": "E5bY_inNcpCR"
      },
      "id": "E5bY_inNcpCR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell6",
      "metadata": {
        "id": "cell6"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    # Initialize the model and training state\n",
        "    model = GPTLanguageModel(\n",
        "        vocab_size=vocab_size,\n",
        "        emb_dim=n_embd,\n",
        "        num_layers=n_layer,\n",
        "        num_heads=n_head,\n",
        "        block_size=block_size,\n",
        "        dropout_rate=dropout_rate\n",
        "    )\n",
        "    dummy_input = jnp.ones((batch_size, block_size), dtype=jnp.int32)\n",
        "    initial_variables = model.init(key, dummy_input, targets=dummy_input, deterministic=False)\n",
        "    params = initial_variables['params']\n",
        "    tx = optax.adamw(learning_rate)\n",
        "    state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "    # Training loop\n",
        "    for iter in range(max_iters):\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss(state)\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        x, y = get_batch('train')\n",
        "        key, dropout_rng = jax.random.split(key)\n",
        "        state = train_step(state, x, y, dropout_rng)\n",
        "\n",
        "    # Generate text from the trained model\n",
        "    context = jnp.zeros((1, 1), dtype=jnp.int32)  # starting with the token index 0\n",
        "    key, gen_rng = jax.random.split(key)\n",
        "    generated = model.generate(state.params, context, max_new_tokens=500, rng=gen_rng)\n",
        "    print(decode(np.array(generated[0])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Code"
      ],
      "metadata": {
        "id": "1ecmfX7hcsOf"
      },
      "id": "1ecmfX7hcsOf"
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Hyperparameters and setup\n",
        "# -----------------------------------------------------------------------------\n",
        "batch_size = 64          # number of independent sequences processed in parallel\n",
        "block_size = 256         # maximum context length for predictions\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "learning_rate = 3e-4\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout_rate = 0.2\n",
        "seed = 1337\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(seed)\n",
        "key = jax.random.PRNGKey(seed)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Data Loading and Preprocessing\n",
        "# -----------------------------------------------------------------------------\n",
        "# Read the text (e.g. tiny Shakespeare)\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Create vocabulary and encoding/decoding functions\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Encode the full text as a numpy array of integers\n",
        "data = np.array(encode(text), dtype=np.int32)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    \"\"\"Generate a small batch of data as JAX arrays of inputs (x) and targets (y).\"\"\"\n",
        "    data_split = train_data if split == 'train' else val_data\n",
        "    ix = np.random.randint(0, len(data_split) - block_size, size=(batch_size,))\n",
        "    x = np.stack([data_split[i : i + block_size] for i in ix])\n",
        "    y = np.stack([data_split[i + 1 : i + block_size + 1] for i in ix])\n",
        "    return jnp.array(x), jnp.array(y)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Model Definition using Flax\n",
        "# -----------------------------------------------------------------------------\n",
        "class Head(nn.Module):\n",
        "    head_size: int\n",
        "    dropout_rate: float = dropout_rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic: bool = False):\n",
        "        # x: (B, T, C)\n",
        "        B, T, _ = x.shape\n",
        "        # Compute key, query, and value projections\n",
        "        k = nn.Dense(self.head_size, use_bias=False)(x)\n",
        "        q = nn.Dense(self.head_size, use_bias=False)(x)\n",
        "        v = nn.Dense(self.head_size, use_bias=False)(x)\n",
        "        scale = 1.0 / jnp.sqrt(self.head_size)\n",
        "        # Compute attention scores\n",
        "        wei = jnp.matmul(q, jnp.swapaxes(k, -2, -1)) * scale  # (B, T, T)\n",
        "        # Create a lower-triangular mask\n",
        "        mask = jnp.tril(jnp.ones((T, T), dtype=bool))\n",
        "        wei = jnp.where(mask, wei, -1e10)\n",
        "        wei = nn.softmax(wei, axis=-1)\n",
        "        wei = nn.Dropout(rate=self.dropout_rate)(wei, deterministic=deterministic)\n",
        "        # Weighted aggregation of the values\n",
        "        out = jnp.matmul(wei, v)  # (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    num_heads: int\n",
        "    head_size: int\n",
        "    emb_dim: int\n",
        "    dropout_rate: float = dropout_rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic: bool = False):\n",
        "        heads = [\n",
        "            Head(self.head_size, dropout_rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "            for _ in range(self.num_heads)\n",
        "        ]\n",
        "        # Concatenate along the feature dimension\n",
        "        out = jnp.concatenate(heads, axis=-1)\n",
        "        out = nn.Dense(self.emb_dim)(out)\n",
        "        out = nn.Dropout(rate=self.dropout_rate)(out, deterministic=deterministic)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    emb_dim: int\n",
        "    dropout_rate: float = dropout_rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic: bool = False):\n",
        "        x = nn.Dense(4 * self.emb_dim)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(self.emb_dim)(x)\n",
        "        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    emb_dim: int\n",
        "    num_heads: int\n",
        "    dropout_rate: float = dropout_rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic: bool = False):\n",
        "        # Self-attention with residual connection\n",
        "        x = x + MultiHeadAttention(\n",
        "            num_heads=self.num_heads,\n",
        "            head_size=self.emb_dim // self.num_heads,\n",
        "            emb_dim=self.emb_dim,\n",
        "            dropout_rate=self.dropout_rate\n",
        "        )(nn.LayerNorm()(x), deterministic=deterministic)\n",
        "        # Feed-forward network with residual connection\n",
        "        x = x + FeedForward(emb_dim=self.emb_dim, dropout_rate=self.dropout_rate)(\n",
        "            nn.LayerNorm()(x), deterministic=deterministic\n",
        "        )\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    vocab_size: int\n",
        "    emb_dim: int\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    block_size: int\n",
        "    dropout_rate: float = dropout_rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, idx, targets=None, deterministic: bool = False):\n",
        "        # idx: (B, T) of token indices\n",
        "        B, T = idx.shape\n",
        "        # Token and positional embeddings\n",
        "        tok_emb = nn.Embed(num_embeddings=self.vocab_size, features=self.emb_dim)(idx)\n",
        "        pos_emb = nn.Embed(num_embeddings=self.block_size, features=self.emb_dim)(\n",
        "            jnp.arange(T)\n",
        "        )\n",
        "        x = tok_emb + pos_emb[None, :, :]\n",
        "        # Transformer blocks\n",
        "        for _ in range(self.num_layers):\n",
        "            x = Block(emb_dim=self.emb_dim, num_heads=self.num_heads, dropout_rate=self.dropout_rate)(\n",
        "                x, deterministic=deterministic\n",
        "            )\n",
        "        x = nn.LayerNorm()(x)\n",
        "        logits = nn.Dense(self.vocab_size)(x)  # (B, T, vocab_size)\n",
        "        if targets is not None:\n",
        "            # Flatten the logits and targets for computing the loss\n",
        "            logits = logits.reshape(-1, self.vocab_size)\n",
        "            targets = targets.reshape(-1)\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
        "        else:\n",
        "            loss = None\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, params, idx, max_new_tokens, rng):\n",
        "        \"\"\"Generate new text tokens given a starting context.\"\"\"\n",
        "        # idx: (B, T)\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop to the last block_size tokens\n",
        "            idx_cond = idx[:, -self.block_size :]\n",
        "            logits, _ = self.apply({'params': params}, idx_cond, deterministic=True)\n",
        "            logits = logits[:, -1, :]  # (B, vocab_size) for the last time step\n",
        "            # Sample from the distribution\n",
        "            next_token = jax.random.categorical(rng, logits)[:, None]\n",
        "            idx = jnp.concatenate([idx, next_token], axis=1)\n",
        "            rng, _ = jax.random.split(rng)\n",
        "        return idx\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Training State and Step Functions\n",
        "# -----------------------------------------------------------------------------\n",
        "class TrainState(train_state.TrainState):\n",
        "    pass\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, x, y, dropout_rng):\n",
        "    def loss_fn(params):\n",
        "        _, loss = model.apply({'params': params}, x, targets=y, deterministic=False, rngs={'dropout': dropout_rng})\n",
        "        return loss\n",
        "    grads = jax.grad(loss_fn)(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(params, x, y):\n",
        "    _, loss = model.apply({'params': params}, x, targets=y, deterministic=True)\n",
        "    return loss\n",
        "\n",
        "def estimate_loss(state):\n",
        "    losses = {'train': [], 'val': []}\n",
        "    for split in ['train', 'val']:\n",
        "        for _ in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            loss = eval_step(state.params, x, y)\n",
        "            losses[split].append(loss)\n",
        "        losses[split] = np.mean([l.item() for l in losses[split]])\n",
        "    return losses\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main Training Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    # Initialize the model and training state\n",
        "    model = GPTLanguageModel(\n",
        "        vocab_size=vocab_size,\n",
        "        emb_dim=n_embd,\n",
        "        num_layers=n_layer,\n",
        "        num_heads=n_head,\n",
        "        block_size=block_size,\n",
        "        dropout_rate=dropout_rate\n",
        "    )\n",
        "    dummy_input = jnp.ones((batch_size, block_size), dtype=jnp.int32)\n",
        "    initial_variables = model.init(key, dummy_input, targets=dummy_input, deterministic=False)\n",
        "    params = initial_variables['params']\n",
        "    tx = optax.adamw(learning_rate)\n",
        "    state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "    # Training loop\n",
        "    for iter in range(max_iters):\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss(state)\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        x, y = get_batch('train')\n",
        "        key, dropout_rng = jax.random.split(key)\n",
        "        state = train_step(state, x, y, dropout_rng)\n",
        "\n",
        "    # Generate text from the trained model\n",
        "    context = jnp.zeros((1, 1), dtype=jnp.int32)  # starting with the token index 0\n",
        "    key, gen_rng = jax.random.split(key)\n",
        "    generated = model.generate(state.params, context, max_new_tokens=500, rng=gen_rng)\n",
        "    print(decode(np.array(generated[0])))\n"
      ],
      "metadata": {
        "id": "HM1d639zcvYX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "cbe57692-c1f6-4126-902c-5c9e2395a65d"
      },
      "id": "HM1d639zcvYX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6297, val loss 4.6451\n",
            "step 1: train loss 4.5821, val loss 4.6249\n",
            "step 2: train loss 4.4477, val loss 4.4833\n",
            "step 3: train loss 4.2935, val loss 4.3219\n",
            "step 4: train loss 4.1183, val loss 4.1443\n",
            "step 5: train loss 3.8985, val loss 3.9255\n",
            "step 6: train loss 3.6706, val loss 3.7072\n",
            "step 7: train loss 3.4779, val loss 3.5138\n",
            "step 8: train loss 3.3642, val loss 3.4008\n",
            "step 9: train loss 3.3840, val loss 3.4204\n",
            "step 10: train loss 3.4494, val loss 3.4882\n",
            "step 11: train loss 3.4496, val loss 3.4850\n",
            "step 12: train loss 3.3940, val loss 3.4297\n",
            "step 13: train loss 3.3479, val loss 3.3825\n",
            "step 14: train loss 3.3302, val loss 3.3633\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2ff3d1c2e7aa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2ff3d1c2e7aa>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2ff3d1c2e7aa>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblock_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# -----------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin, device)\u001b[0m\n\u001b[1;32m   5564\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5565\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected input type for array: {type(object)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5566\u001b[0;31m   out_array: Array = lax_internal._convert_element_type(\n\u001b[0m\u001b[1;32m   5567\u001b[0m       out, dtype, weak_type=weak_type, sharding=sharding)\n\u001b[1;32m   5568\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type, sharding, warn_on_complex_to_real_cast)\u001b[0m\n\u001b[1;32m   1412\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moperand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m     return convert_element_type_p.bind(\n\u001b[0m\u001b[1;32m   1415\u001b[0m         \u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         sharding=sharding)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    500\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_canonicalization\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanonicalize_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_true_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_true_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36m_true_bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0mtrace_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mtrace_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type_bind_with_trace\u001b[0;34m(trace, args, params)\u001b[0m\n\u001b[1;32m   4369\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_convert_element_type_bind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4370\u001b[0m   \u001b[0msharding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sharding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4371\u001b[0;31m   \u001b[0moperand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_element_type_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4372\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msharding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msharding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_concrete\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4373\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdef_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, args, params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mcheck_eval_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_jit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_thread_local_state_disable_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_jit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_thread_local_state_disable_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}